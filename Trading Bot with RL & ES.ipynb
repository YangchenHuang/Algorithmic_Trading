{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YangchenHuang/Algorithmic_Trading/blob/master/Trading%20Bot%20with%20RL%20%26%20ES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tajN4T3GoQJK",
        "colab_type": "code",
        "outputId": "13fe70a4-3c2c-4290-8389-b22a58233159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Use Google Colab as the development environment\n",
        "!pip install bayesian-optimization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=6e20b134d9a6207e4eff2c0cecdf60a5dc57d7a5f0b87236a595d0e95c97fba3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ihq_l9Po0O7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Omn--cKo1dh",
        "colab_type": "code",
        "outputId": "0703ad79-42d7-48d6-c5b5-fabc293b1fb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Get the data from github dataset\n",
        "!rm -rf Algorithmic_Trading\n",
        "!git clone https://github.com/YangchenHuang/Algorithmic_Trading"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Algorithmic_Trading'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 139 (delta 21), reused 0 (delta 0), pack-reused 90\u001b[K\n",
            "Receiving objects: 100% (139/139), 26.02 MiB | 8.48 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWsJRKEYpRbM",
        "colab_type": "code",
        "outputId": "f8a74c59-93c4-4cdd-d861-0e8995bd21a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df = pd.read_csv('./Algorithmic_Trading/dataset/NVDA.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PERMNO</th>\n",
              "      <th>date</th>\n",
              "      <th>VOL</th>\n",
              "      <th>NUMTRD</th>\n",
              "      <th>PRC</th>\n",
              "      <th>OPENPRC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86580</td>\n",
              "      <td>1/3/2007</td>\n",
              "      <td>19273722</td>\n",
              "      <td>67875</td>\n",
              "      <td>24.053333</td>\n",
              "      <td>24.713333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>86580</td>\n",
              "      <td>1/4/2007</td>\n",
              "      <td>15957279</td>\n",
              "      <td>53389</td>\n",
              "      <td>23.940000</td>\n",
              "      <td>23.966667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>86580</td>\n",
              "      <td>1/5/2007</td>\n",
              "      <td>20729060</td>\n",
              "      <td>75741</td>\n",
              "      <td>22.440000</td>\n",
              "      <td>23.373333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>86580</td>\n",
              "      <td>1/8/2007</td>\n",
              "      <td>10955217</td>\n",
              "      <td>43714</td>\n",
              "      <td>22.606667</td>\n",
              "      <td>22.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>86580</td>\n",
              "      <td>1/9/2007</td>\n",
              "      <td>12750524</td>\n",
              "      <td>48630</td>\n",
              "      <td>22.166667</td>\n",
              "      <td>22.640000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PERMNO      date       VOL  NUMTRD        PRC    OPENPRC\n",
              "0   86580  1/3/2007  19273722   67875  24.053333  24.713333\n",
              "1   86580  1/4/2007  15957279   53389  23.940000  23.966667\n",
              "2   86580  1/5/2007  20729060   75741  22.440000  23.373333\n",
              "3   86580  1/8/2007  10955217   43714  22.606667  22.520000\n",
              "4   86580  1/9/2007  12750524   48630  22.166667  22.640000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy1dL3eHpbBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Cleaning, create two list\n",
        "data = df[['PRC', 'VOL', 'OPENPRC']].to_numpy().flatten()\n",
        "data = data.tolist()\n",
        "data = [abs(_) for _ in data]\n",
        "close = df['PRC'].tolist()\n",
        "close = [abs(_) for _ in close]\n",
        "# Train/Test Data Split\n",
        "l = len(close) - 1\n",
        "test_size=l//2\n",
        "dev_size=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-DoxI3eqF7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is to get the historical window of data as the state at time t\n",
        "def get_state(data, t, n):\n",
        "    d = t - n + 1\n",
        "    block = data[d*3 : (t + 1)*3] if d >= 0 else -d * [data[0], data[1], data[2]] + data[0 : (t + 1)*3]\n",
        "    res = []\n",
        "    for i in range(n - 1):\n",
        "        for j in range(3):\n",
        "            if block[i*3+j]==0:\n",
        "                res.append(0)\n",
        "            else:\n",
        "                res.append((block[(i + 1)*3+j] - block[i*3+j])/block[i*3+j])\n",
        "    return np.array([res])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WFcNQEApdr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Deep_Evolution_Strategy:\n",
        "\n",
        "    inputs = None\n",
        "\n",
        "    def __init__(\n",
        "        self, weights, reward_function, population_size, sigma, learning_rate, trade_train, buy, model\n",
        "    ):\n",
        "        self.weights = weights\n",
        "        self.reward_function = reward_function\n",
        "        self.population_size = population_size\n",
        "        self.sigma = sigma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.fsigma = sigma\n",
        "        self.trade_train = trade_train\n",
        "        self.buy = buy\n",
        "        self.model = model\n",
        "        self.wc = weights\n",
        "    \n",
        "    # Random Mutation\n",
        "    def _get_weight_from_population(self, weights, population):\n",
        "        weights_population = []\n",
        "        for index, i in enumerate(population):\n",
        "            jittered = self.sigma * i\n",
        "            weights_population.append(weights[index] + jittered)\n",
        "        return weights_population\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    # Training part\n",
        "    def train(self, epoch = 100, print_every = 1):\n",
        "        lasttime = time.time()\n",
        "        reward_list=[]\n",
        "        max_validate=0\n",
        "        best_model=self.model\n",
        "        for i in range(epoch):\n",
        "            population = []\n",
        "            rewards = np.zeros(self.population_size)\n",
        "            # Randomly generate populations\n",
        "            for k in range(self.population_size):\n",
        "                x = []\n",
        "                for w in self.weights:\n",
        "                    x.append(np.random.randn(*w.shape))\n",
        "                population.append(x)\n",
        "            # Get reward for each one in the population\n",
        "            for k in range(self.population_size):\n",
        "                weights_population = self._get_weight_from_population(\n",
        "                    self.weights, population[k]\n",
        "                )\n",
        "                rewards[k] = self.reward_function(weights_population)\n",
        "            # Softmax Weighted reward\n",
        "            e_rewards = np.exp(rewards - np.max(rewards))\n",
        "            e_rewards = e_rewards / e_rewards.sum(axis=0)\n",
        "            for index, w in enumerate(self.weights):\n",
        "                A = np.array([p[index] for p in population])\n",
        "                self.wc[index] = (\n",
        "                    w\n",
        "                    + self.learning_rate\n",
        "                    * np.dot(A.T, e_rewards).T\n",
        "                )\n",
        "            now_reward=self.reward_function(self.wc)\n",
        "            # Mutation rate heauristic\n",
        "            if i==0:\n",
        "                final_reward = now_reward \n",
        "            if i>0:\n",
        "                final_reward = reward_list[i-1]\n",
        "            if i>0 and (self.sigma > self.fsigma/4 or now_reward > reward_list[i-1]):\n",
        "                self.weights = self.wc\n",
        "                final_reward = now_reward\n",
        "            if i>0:\n",
        "                if final_reward<=0:\n",
        "                    self.sigma = self.fsigma\n",
        "                if self.sigma > self.fsigma/2:\n",
        "                    if final_reward>max(reward_list[max(0, i-5):i]):\n",
        "                        self.sigma=self.sigma*np.clip((final_reward-max(reward_list))/(final_reward+1e-10),0.95, 0.999)\n",
        "                    if final_reward<np.average(reward_list):\n",
        "                        self.sigma=min(self.sigma*np.clip(np.average(reward_list)/(np.average(reward_list)-final_reward+1e-10),1.001, 1.05),self.fsigma)\n",
        "                elif self.sigma > self.fsigma/4:\n",
        "                    if final_reward>max(reward_list[max(0, i-10):i]):\n",
        "                        self.sigma=self.sigma*np.clip((final_reward-max(reward_list))/(final_reward+1e-10),0.9, 0.95)\n",
        "                elif self.sigma > self.fsigma/10:\n",
        "                    if final_reward>max(reward_list):\n",
        "                        self.sigma=self.sigma*(np.random.random()*0.4+0.5)\n",
        "                else:\n",
        "                    if final_reward>max(reward_list):\n",
        "                        self.sigma=self.sigma*(np.random.random()*0.5)\n",
        "            # print(self.sigma)\n",
        "            reward_list.append(final_reward)\n",
        "            # v=self.validate()\n",
        "            # if (i + 1) % print_every == 0:\n",
        "            #     print(\n",
        "            #         'iter %d. reward: %f validate: %f %%'\n",
        "            #         % (i + 1, final_reward, v)\n",
        "            #     )\n",
        "            if self.sigma<10e-5:\n",
        "                print(\n",
        "                    'iter %d. reward: %f'\n",
        "                    % (i + 1, final_reward)\n",
        "                )\n",
        "                self.trade_train(self.weights)\n",
        "                self.buy(self.weights)\n",
        "                break\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(\n",
        "                    'iter %d. reward: %f'\n",
        "                    % (i + 1, final_reward)\n",
        "                )\n",
        "            # if v > max_validate:\n",
        "            #     best_model.set_weights=self.weights\n",
        "            # if i==epoch-1:\n",
        "            #     self.buy()\n",
        "            \n",
        "        print('time taken to train:', time.time() - lasttime, 'seconds')\n",
        "\n",
        "# Neural Network Structure\n",
        "class Model:\n",
        "    def __init__(self, input_size, layer_size, output_size):\n",
        "        self.weights = [\n",
        "            np.random.randn(input_size, layer_size)*0.01,\n",
        "            np.random.randn(layer_size, output_size)*0.01,\n",
        "            np.random.randn(layer_size+1, 1)*0.01,\n",
        "            np.zeros((1, layer_size)),\n",
        "            np.zeros((1, output_size)),\n",
        "            np.zeros((1,1))\n",
        "        ]\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        feed = np.dot(inputs, self.weights[0]) + self.weights[-3]\n",
        "        a1=np.tanh(feed)\n",
        "        # Buy/Sell Signal\n",
        "        decision = np.dot(a1, self.weights[1]) + self.weights[-2]\n",
        "        z = np.append(np.argmax(decision[0]), a1)\n",
        "        # Trade Shares\n",
        "        trade = np.dot(z, self.weights[2]) + self.weights[-1]\n",
        "        trade = trade[0][0]\n",
        "        if np.isnan(trade):\n",
        "            trade = 0\n",
        "        trade = max(trade, 0)\n",
        "        return decision, trade\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.weights = weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6qPWavmphuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trading Agent\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        population_size,\n",
        "        sigma,\n",
        "        learning_rate,\n",
        "        model,\n",
        "        money,\n",
        "        skip,\n",
        "        window_size,\n",
        "    ):\n",
        "        self.window_size = window_size\n",
        "        self.skip = skip\n",
        "        self.POPULATION_SIZE = population_size\n",
        "        self.SIGMA = sigma\n",
        "        self.LEARNING_RATE = learning_rate\n",
        "        self.model = model\n",
        "        self.initial_money = money\n",
        "        self.es = Deep_Evolution_Strategy(\n",
        "            self.model.get_weights(),\n",
        "            self.get_reward,\n",
        "            self.POPULATION_SIZE,\n",
        "            self.SIGMA,\n",
        "            self.LEARNING_RATE,\n",
        "            self.trade_train,\n",
        "            self.buy,\n",
        "            self.model\n",
        "        )\n",
        "\n",
        "    def act(self, sequence):\n",
        "        decision, buy = self.model.predict(np.array(sequence))\n",
        "        return np.argmax(decision[0]), int(buy)\n",
        "\n",
        "    # Reward Function\n",
        "    def get_reward(self, weights):\n",
        "        initial_money = self.initial_money\n",
        "        starting_money = initial_money\n",
        "        self.model.weights = weights\n",
        "        state = get_state(data, 30, self.window_size + 1)\n",
        "        inventory = []\n",
        "        quantity = 0\n",
        "        pnl = []\n",
        "        maxdown = []\n",
        "        loss_penalty = 1\n",
        "        risk_penalty = 1\n",
        "        now_money = starting_money\n",
        "        # final_money = starting_money\n",
        "        for t in range(30, l-test_size-dev_size, self.skip):\n",
        "            action, trade = self.act(state)\n",
        "            next_state = get_state(data, t + 1, self.window_size + 1)\n",
        "            max_buy = min(trade, starting_money//close[t]//2, initial_money//close[t])\n",
        "            if action == 1 and initial_money >= close[t]:\n",
        "                buy_units = max_buy\n",
        "                total_buy = buy_units * close[t]\n",
        "                initial_money -= total_buy\n",
        "                inventory.append(total_buy)\n",
        "                quantity += buy_units\n",
        "            elif action == 2 and len(inventory) > 0:\n",
        "                bought_price = inventory.pop(0)\n",
        "                sell_units = quantity\n",
        "                quantity -= sell_units\n",
        "                total_sell = sell_units * close[t]\n",
        "                initial_money += total_sell\n",
        "                # final_money = initial_money\n",
        "            now_money=initial_money+quantity*close[t]\n",
        "            pnl.append(((now_money - starting_money) / starting_money) * 100)\n",
        "            if t>=60:\n",
        "                maxdown.append(max(pnl[t-60:t-31])-pnl[t-31])\n",
        "            state = next_state\n",
        "        # Loss penalty heuristic\n",
        "        if now_money>starting_money and min(pnl)<0:\n",
        "            loss_penalty = max(1, 1/(1+min(pnl)*0.01))\n",
        "        # Risk penalty heuristic\n",
        "        if now_money>starting_money and max(maxdown)>0:\n",
        "            risk_penalty = max(1, 1/(1-max(maxdown)*0.01))\n",
        "        return ((now_money - starting_money) / starting_money) * 100/ (loss_penalty * risk_penalty)\n",
        "\n",
        "    def fit(self, iterations, checkpoint):\n",
        "        self.es.train(iterations, print_every = checkpoint)\n",
        "\n",
        "    # Trading Strategy on training set\n",
        "    def trade_train(self, weights):\n",
        "        initial_money = self.initial_money\n",
        "        self.model.weights = weights\n",
        "        state = get_state(data, 30, self.window_size + 1)\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        quantity = 0\n",
        "        now_money = initial_money\n",
        "        pnl=[]\n",
        "        # final_money = starting_money\n",
        "        for t in range(30, l-test_size, self.skip):\n",
        "            action, trade = self.act(state)\n",
        "            next_state = get_state(data, t + 1, self.window_size + 1)\n",
        "            max_buy = min(trade, starting_money//close[t]//2, initial_money//close[t])\n",
        "            if action == 1 and initial_money >= close[t]:\n",
        "                buy_units = max_buy\n",
        "                total_buy = buy_units * close[t]\n",
        "                initial_money -= total_buy\n",
        "                inventory.append(total_buy)\n",
        "                quantity += buy_units\n",
        "                states_buy.append(t)\n",
        "            elif action == 2 and len(inventory) > 0:\n",
        "                bought_price = inventory.pop(0)\n",
        "                sell_units = quantity\n",
        "                quantity -= sell_units\n",
        "                total_sell = sell_units * close[t]\n",
        "                initial_money += total_sell\n",
        "                # final_money = initial_money\n",
        "                states_sell.append(t)\n",
        "            now_money=initial_money+quantity*close[t]\n",
        "            pnl.append(((now_money - starting_money) / starting_money) * 100)\n",
        "            state = next_state\n",
        "        invest = ((now_money - starting_money) / starting_money) * 100\n",
        "        print(\n",
        "            '\\ntotal gained %f, total investment %f %%'\n",
        "            % (now_money - starting_money, invest)\n",
        "        )\n",
        "        plt.figure(figsize = (20, 10))\n",
        "        plt.plot(close, label = 'true close', c = 'g')\n",
        "        plt.plot(\n",
        "            close, 'X', label = 'predict buy', markevery = states_buy, c = 'b'\n",
        "        )\n",
        "        plt.plot(\n",
        "            close, 'o', label = 'predict sell', markevery = states_sell, c = 'r'\n",
        "        )\n",
        "        plt.legend()\n",
        "        plt.figure(figsize = (20, 10))\n",
        "        plt.plot(pnl, label = 'pnl', c = 'r')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # Trading Strategy on test set\n",
        "    def buy(self, weights):\n",
        "        initial_money = self.initial_money\n",
        "        self.model.weights = weights\n",
        "        state = get_state(data, l-test_size, self.window_size + 1)\n",
        "        starting_money = initial_money\n",
        "        states_sell = []\n",
        "        states_buy = []\n",
        "        inventory = []\n",
        "        quantity = 0\n",
        "        now_money = initial_money\n",
        "        pnl=[]\n",
        "        # final_money = starting_money\n",
        "        for t in range(l-test_size, l, self.skip):\n",
        "            action, trade = self.act(state)\n",
        "            next_state = get_state(data, t + 1, self.window_size + 1)\n",
        "            max_buy = min(trade, starting_money//close[t]//2, initial_money//close[t])\n",
        "            if action == 1 and initial_money >= close[t]:\n",
        "                buy_units = max_buy\n",
        "                total_buy = buy_units * close[t]\n",
        "                initial_money -= total_buy\n",
        "                inventory.append(total_buy)\n",
        "                quantity += buy_units\n",
        "                states_buy.append(t)\n",
        "                print(\n",
        "                    'day %d: buy %d units at price %f, total balance %f'\n",
        "                    % (t, buy_units, total_buy, initial_money)\n",
        "                )\n",
        "            elif action == 2 and len(inventory) > 0:\n",
        "                bought_price = inventory.pop(0)\n",
        "                sell_units = quantity\n",
        "                quantity -= sell_units\n",
        "                total_sell = sell_units * close[t]\n",
        "                initial_money += total_sell\n",
        "                # final_money = initial_money\n",
        "                states_sell.append(t)\n",
        "                print(\n",
        "                    'day %d, sell %d units at price %f, total balance %f,'\n",
        "                    % (t, sell_units, total_sell, initial_money)\n",
        "                )\n",
        "            now_money=initial_money+quantity*close[t]\n",
        "            pnl.append(((now_money - starting_money) / starting_money) * 100)\n",
        "            state = next_state\n",
        "        invest = ((now_money - starting_money) / starting_money) * 100\n",
        "        print(\n",
        "            '\\ntotal gained %f, total investment %f %%'\n",
        "            % (now_money - starting_money, invest)\n",
        "        )\n",
        "        plt.figure(figsize = (20, 10))\n",
        "        plt.plot(close, label = 'true close', c = 'g')\n",
        "        plt.plot(\n",
        "            close, 'X', label = 'predict buy', markevery = states_buy, c = 'b'\n",
        "        )\n",
        "        plt.plot(\n",
        "            close, 'o', label = 'predict sell', markevery = states_sell, c = 'r'\n",
        "        )\n",
        "        plt.legend()\n",
        "        plt.figure(figsize = (20, 10))\n",
        "        plt.plot(pnl, label = 'pnl', c = 'r')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvbWnGUf-lg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_agent(\n",
        "    window_size, skip, population_size, sigma, learning_rate, size_network\n",
        "):\n",
        "    model = Model(window_size*3, size_network, 3)\n",
        "    agent = Agent(\n",
        "        population_size,\n",
        "        sigma,\n",
        "        learning_rate,\n",
        "        model,\n",
        "        100000,\n",
        "        skip,\n",
        "        window_size,\n",
        "    )\n",
        "    agent.fit(5, 5)\n",
        "    return agent.es.reward_function(agent.es.weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk8nbizbplv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Used for Bayesian Optimization\n",
        "def find_best_agent(\n",
        "    window_size, skip, population_size, sigma, learning_rate, size_network\n",
        "):\n",
        "    global accbest\n",
        "    param = {\n",
        "        'window_size': int(np.around(window_size)),\n",
        "        'skip': int(np.around(skip)),\n",
        "        'population_size': int(np.around(population_size)),\n",
        "        'sigma': max(min(sigma, 1), 0.0001),\n",
        "        'learning_rate': max(min(learning_rate, 0.5), 0.000001),\n",
        "        'size_network': int(np.around(size_network)),\n",
        "    }\n",
        "    print('\\nSearch parameters %s' % (param))\n",
        "    investment = best_agent(**param)\n",
        "    print('stop after 100 iteration with investment %f' % (investment))\n",
        "    if investment > accbest:\n",
        "        costbest = investment\n",
        "    return investment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAb_SM_dpvsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bayesian Optimization to find the best-in-class hyperparameters\n",
        "accbest = 0.0\n",
        "NN_BAYESIAN = BayesianOptimization(\n",
        "    find_best_agent,\n",
        "    {\n",
        "        'window_size': (7, 30),\n",
        "        'skip': (1, 1),\n",
        "        'population_size': (5, 25),\n",
        "        'sigma': (0.2, 0.5),\n",
        "        'learning_rate': (0.2, 0.5),\n",
        "        'size_network': (100, 1000),\n",
        "    },\n",
        ")\n",
        "NN_BAYESIAN.maximize(init_points = 30, n_iter = 20, acq = 'ei', xi = 0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uOjwwd9qVR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reinforcement Learning Using manually selected hyperparameters\n",
        "model = Model(input_size = 30*3, \n",
        "              layer_size = 500, \n",
        "              output_size = 3)\n",
        "agent = Agent(population_size = 15, \n",
        "              sigma = 0.5, \n",
        "              learning_rate = 0.5, \n",
        "              model = model, \n",
        "              money = 100000,  \n",
        "              skip = 1, \n",
        "              window_size = 30)\n",
        "agent.fit(500, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlPNINFrqSL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reinforcement Learning Using Bayesian selected hyperparameters\n",
        "model = Model(input_size = int(np.around(NN_BAYESIAN.max['params']['window_size']))*3, \n",
        "              layer_size = int(np.around(NN_BAYESIAN.max['params']['size_network'])), \n",
        "              output_size = 3)\n",
        "agent = Agent(population_size = int(np.around(NN_BAYESIAN.max['params']['population_size'])), \n",
        "              sigma = NN_BAYESIAN.max['params']['sigma'], \n",
        "              learning_rate = NN_BAYESIAN.max['params']['learning_rate'], \n",
        "              model = model, \n",
        "              money = 100000, \n",
        "              skip = int(np.around(NN_BAYESIAN.max['params']['skip'])), \n",
        "              window_size = int(np.around(NN_BAYESIAN.max['params']['window_size'])))\n",
        "agent.fit(500, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}